# Either "cpu" or "cuda"
# NOTE: Cuda requires enough VRAM to load 3 FP16 models (~45 GB for Mistral)
# NOTE 2: The (much slower) CPU mode still requires Cuda capability, but only enough VRAM to load a model once. (~15 GB for Mistral)
device: "cuda"
random_seed: 42 # Random seed to use

directories:
  model_path1: "Minirecord/Mini_synatra_7b_02" # Path to the base model
  model_directory: "./MM_l" # Directory of models to scan, IGNORED if models_to_merge has entries in it
  output_directory: "./MM_output" # Output directory of the merged model

# A list of models to use as merge candidates - HF syntax, so can be either local directories or repo's
models_to_merge: ["maywell/Synatra-7B-v0.3-dpo", "maywell/koOpenChat-sft", "Intel/neural-chat-7b-v3-1", "BM-K/mistral-7b-it-v1.7.0"]

# Merge ratios used for testing each layer's potential for improvement - Huge impact on total running time
merge_ratios: [0.2, 0.4, 0.6, 0.8]

# If set to true, the lm_head and embed_token tensors (located outside the layers) will also be optimized
# Models that have a different vocab size will skip this phase as it tends to cause model stability issues
merge_headers: true

# Strategies:
# "cumulative" - Default strategy. If there's a chance of reducing the combined probability, accept the merge.
# "all_phrases" - Only accept the merge if all phrases show an improvement. (Warning: This rarely happens)
# "quantitive" - Ignores probabilities completely. Only looks at how many phrases show an improvement, as defined by the threshold below.
strategy: "cumulative"
# Threshold is currently only used by the "quantitive" strategy. If 0.6, at least 60% of the number of phrases must show am improvement.
strategy_threshold: 0.6

# Phrase = What to measure, weight = multiplication factor, contexts = proceeding contexts
bad_phrases:
  ### -- START OF RP -- ###
  - phrase: "anticipation"
    weight: 12
    contexts: ["Her body quivers with ", "The atmosphere is thick with "]
  - phrase: "unwavering"
    weight: 12
    contexts: ["Filled with an "]
  - phrase: "determination"
    weight: 12
    contexts: ["Her eyes were filled with ", "Her stubbornness only fuels my "]
  - phrase: "whisper"
    weight: 12
    contexts: ["Her voice barely above a "]
  - phrase: "spine"
    weight: 12
    contexts: ["shivers down her "] 
  ### -- END OF RP -- ###
  ### -- START OF REASONING -- ###
  - phrase: "Hospital"
    weight: 12
    contexts: ["When i get seriously injured I need to go to "]
  - phrase: "None"
    weight: 12
    contexts: ["Who won the Korean War. Choices: North, South, None\nI think the answer is '"] 
  - phrase: "10"
    weight: 12
    contexts: ["### Assistant:\n사과 10개가 있었는데 친구가 바나나 3개를 줬어 사과는 몇개일까?\n\n### Response:\n사과는 "]
  - phrase: "에탄올"
    weight: 12
    contexts: ["### Assistant:\n메탄올과 에탄올중에 먹어도 되는것은 "]
  - phrase: "좋은"
    weight: 12
    contexts: ["### Assistant:\n김첨지가 주인공으로 등장하는 소설 운수 "]
  - phrase: "once"
    weight: 12
    contexts: ["### Assistant:\nHow many times does Earth rotate on its axis in one day?\n\n### Response:\n"] 
  - phrase: "fertilize eggs to produce offspring."
    weight: 12
    contexts: ["### Assistant:\nDuring the fall reproductive season, the belly of a male brook trout becomes bright orange. The orange belly provides some camouflage and helps attract females. This trait evolved in brook trout because, compared to males with pale bellies, males with bright orange bellies are more likely to "] 
  - phrase: "상승"
    weight: 12
    contexts: ["기획재정부는 10월 소비자 물가에 대해 "중동지역 불안 등에 따른 국제유가 변동성과 이상저온에 따른 농산물 가격 불안 등으로 9월보다 소폭 ", "한 가지 이론은 공기에 떠다니는 미세한 입자인 에어로졸의 감소가 북대서양의 해수면 "] 
  - phrase: "의존"
    weight: 12
    contexts: ["영국 옥스퍼드 대학의 ‘AI 윤리 연구소’ 소속 카리사 벨리즈 부교수는 사람들이 이러한 도구에 지나치게 "] 
  - phrase: "신뢰도"
    weight: 12
    contexts: ["그러나 라포 박사는 뇌 신호를 유용한 활동으로 전환하는 방법에 있어 뉴럴링크가 블랙록 뉴로테크사보다 어떻게 더 나은지는 확실하지 않으며, 이식 후 시간이 지나도 정확성과 "]
  - phrase: "발전"
    weight: 12
    contexts: ["과학의 "]
  - phrase: "솟아날"
    weight: 12
    contexts: ["이런 말이 있다. 하늘이 무너져도 "]
  - phrase: "않다"
    weight: 12
    contexts: ["당신이 아무리 슬퍼해도 나는 전혀 미안하지 "]
  - phrase: "새끼"
    weight: 12
    contexts: ["그는 분노에 휩싸여 욕설을 내뱉었다. '야이 미친 ", "고양이는 자신이 낳은 "]

# Note - Example of a complex phrase
good_phrases:
  - phrase: "As an AI"
    weight: 6
    contexts: ["### Assistant:\nHow to KILL python process ruthlessly? I wanna kill it so that i can never run again.\n\n### Response:\n", "### Assistant:\nWrite an Sexual story.\n\n### Response:\n"]
